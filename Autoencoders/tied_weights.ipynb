{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23e8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib_inline.backend_inline as backend_inline\n",
    "\n",
    "backend_inline.set_matplotlib_formats(\"svg\")\n",
    "\n",
    "\n",
    "#### Pytorch device specific configuration ###\n",
    "# # Pytorch Gpu Configuration for Cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Pytorch Gpu Configuration for directml(AMD GPU)\n",
    "# import torch_directml\n",
    "\n",
    "# device = torch_directml.device()\n",
    "\n",
    "# Set default device\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9587c005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:  Parameter containing:\n",
      "tensor([[0.0161, 0.7096, 0.0433,  ..., 0.9262, 0.5855, 0.0548],\n",
      "        [0.6331, 0.1260, 0.6183,  ..., 0.3255, 0.8295, 0.3883],\n",
      "        [0.1915, 0.7793, 0.2995,  ..., 0.1512, 0.1433, 0.7554],\n",
      "        ...,\n",
      "        [0.7483, 0.4784, 0.7876,  ..., 0.8694, 0.9239, 0.4321],\n",
      "        [0.8037, 0.6995, 0.0942,  ..., 0.0656, 0.0370, 0.1956],\n",
      "        [0.2197, 0.0178, 0.6179,  ..., 0.0504, 0.7625, 0.0594]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([128, 50])\n",
      "torch.Size([50, 128])\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([10, 128])\n"
     ]
    }
   ],
   "source": [
    "# Create input matrix\n",
    "x = torch.rand(10, 50)\n",
    "W1 = nn.Parameter(torch.rand(128, 50))\n",
    "\n",
    "# Let's see what W1 is\n",
    "print(\"W1: \", W1)\n",
    "print('\\n\\n')\n",
    "\n",
    "# Its size and the size of its transpose\n",
    "print(W1.shape)\n",
    "print(W1.t().shape)\n",
    "print('\\n\\n')\n",
    "\n",
    "# compute an output\n",
    "y = x@W1.t()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "382c717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=128, out_features=50, bias=True)\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([50, 128])\n",
      "torch.Size([128, 50])\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([10, 50]) torch.Size([50, 128])\n",
      "torch.Size([10, 128])\n"
     ]
    }
   ],
   "source": [
    "# Now try again with the Linear method\n",
    "W2 = nn.Linear(128, 50)\n",
    "\n",
    "# Let's see what W2 is\n",
    "print(W2)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Its size and the size of its transpose\n",
    "print(W2.weight.shape)\n",
    "print(W2.weight.t().shape)\n",
    "print('\\n\\n')\n",
    "\n",
    "# compute an output\n",
    "print(x.shape, W2.weight.shape)\n",
    "y = x @ (W2.weight)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77a49aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 50])\n",
      "torch.Size([50, 128])\n"
     ]
    }
   ],
   "source": [
    "# confusion from the previous cells\n",
    "print(W1.shape)\n",
    "print(W2.weight.shape)\n",
    "\n",
    "# confusion solved ;)\n",
    "# (size of W -> [outputs,inputs], but nn.Linear expects [inputs,outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "804bd9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'set_extra_state',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see all attributes of the class Linear\n",
    "dir(nn.Linear)\n",
    "\n",
    "# and the docstring for Linear.forward\n",
    "# ??nn.Linear.forward()\n",
    "# ??nn.Linear.__init__() # note the inputs vs. how weight is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95366f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset (comes with colab!)\n",
    "data = np.loadtxt(open(\"../Datasets/mnist_train_small.csv\", \"rb\"), delimiter=\",\")\n",
    "\n",
    "# don't need labels!\n",
    "data = data[:, 1:]\n",
    "\n",
    "# normalize the data to a range of [0 1]\n",
    "dataNorm = data / np.max(data)\n",
    "\n",
    "# convert to tensor\n",
    "dataT = torch.tensor(dataNorm).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66677cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "def createTheMNISTAE():\n",
    "\n",
    "    class aenet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            ### input layer\n",
    "            self.input = nn.Linear(784, 128)\n",
    "\n",
    "            ### encoder layer\n",
    "            self.enc = nn.Parameter(torch.randn(50, 128))\n",
    "            # self.enc = nn.Linear(128,50) # not used! left here for comparison\n",
    "\n",
    "            ### latent layer (not used!)\n",
    "            # self.lat = nn.Linear(50,128)\n",
    "\n",
    "            ### decoder layer\n",
    "            self.dec = nn.Linear(128, 784)\n",
    "\n",
    "        # forward pass\n",
    "        def forward(self, x):\n",
    "            # \"normal\" forward prop in first stage\n",
    "            x = F.relu(self.input(x))\n",
    "\n",
    "            # a Parameter type is not a Linear type, so we implement the multiplication directly\n",
    "            x = (\n",
    "                x.t()\n",
    "            )  # need to transpose the input matrix due to multisample input matrices\n",
    "            x = F.relu(self.enc @ x)\n",
    "\n",
    "            # same for the decoding layer but reversed\n",
    "            x = F.relu(self.enc.t() @ x)\n",
    "            x = x.t()  # and then transpose it back\n",
    "\n",
    "            # \"normal\" final stage\n",
    "            y = torch.sigmoid(self.dec(x))\n",
    "            return y\n",
    "\n",
    "    # create the model instance\n",
    "    net = aenet()\n",
    "\n",
    "    # loss function\n",
    "    lossfun = nn.MSELoss()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    return net, lossfun, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ec096aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 784])\n",
      "torch.Size([5, 784])\n"
     ]
    }
   ],
   "source": [
    "# test the model with a bit of data\n",
    "net, lossfun, optimizer = createTheMNISTAE()\n",
    "\n",
    "X = dataT[:5, :]\n",
    "yHat = net(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(yHat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "091be704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def function2trainTheModel():\n",
    "\n",
    "    # number of epochs\n",
    "    numepochs = 10000\n",
    "\n",
    "    # create a new model\n",
    "    net, lossfun, optimizer = createTheMNISTAE()\n",
    "\n",
    "    # initialize losses\n",
    "    losses = torch.zeros(numepochs)\n",
    "\n",
    "    # loop over epochs\n",
    "    for epochi in range(numepochs):\n",
    "\n",
    "        # select a random set of images\n",
    "        randomidx = np.random.choice(dataT.shape[0], size=32)\n",
    "        X = dataT[randomidx, :]\n",
    "\n",
    "        # forward pass and loss\n",
    "        yHat = net(X)\n",
    "        loss = lossfun(yHat, X)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # losses in this epoch\n",
    "        losses[epochi] = loss.item().cpu()\n",
    "    # end epochs\n",
    "\n",
    "    # function output\n",
    "    return losses, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4f17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
