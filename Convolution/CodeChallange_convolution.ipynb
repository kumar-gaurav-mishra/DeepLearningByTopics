{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2442,
     "status": "ok",
     "timestamp": 1625124891642,
     "user": {
      "displayName": "Mike X Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhjXSur8ulydyiQQEh2U6pYGIhDN22fqYYNNDg49A=s64",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib_inline.backend_inline as backend_inline\n",
    "\n",
    "backend_inline.set_matplotlib_formats(\"svg\")\n",
    "\n",
    "\n",
    "#### Pytorch device specific configuration ###\n",
    "# # Pytorch Gpu Configuration for Cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Pytorch Gpu Configuration for directml(AMD GPU)\n",
    "# import torch_directml\n",
    "\n",
    "# device = torch_directml.device()\n",
    "\n",
    "# Set default device\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HOkOefftqyg"
   },
   "source": [
    "# Sample problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiLI77Lh9yms"
   },
   "source": [
    "### Convolve an image of size 1x256x256 to produce a 1x252x84 result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1625124892032,
     "user": {
      "displayName": "Mike X Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhjXSur8ulydyiQQEh2U6pYGIhDN22fqYYNNDg49A=s64",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "VhIKo0_iaGz2",
    "outputId": "39ac054e-2456-47e3-87cf-6381014e3808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected size: [  1 252  84]\n",
      "Empirical size: [252, 84]\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "inChans = 1  # RGB\n",
    "imsize = [256, 256]\n",
    "outChans = 1\n",
    "krnSize = 7  # should be an odd number\n",
    "stride = (1, 3)\n",
    "padding = 1\n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans, outChans, krnSize, stride, padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1, inChans, imsize[0], imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans, 0, 0], dtype=int)\n",
    "expectSize[1] = np.floor((imsize[0] + 2 * padding - krnSize) / stride[0]) + 1\n",
    "expectSize[2] = np.floor((imsize[1] + 2 * padding - krnSize) / stride[1]) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f\"Expected size: {expectSize}\")\n",
    "print(f\"Empirical size: {list(empSize)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCWyuNySDagy"
   },
   "source": [
    "# Real problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bteB_P10DSCM"
   },
   "source": [
    "### 1) Convolve an image of size 3x64x64 to produce a 10x28x28 result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_8CM5aVADSCN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected size: [10 28 28]\n",
      "Empirical size: [10, 28, 28]\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "inChans  = 3\n",
    "imsize   = [64, 64]\n",
    "outChans = 10\n",
    "krnSize  = 12\n",
    "stride   = (2, 2)\n",
    "padding  = 1\n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1,inChans,imsize[0],imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans,0,0],dtype=int)\n",
    "expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n",
    "expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f'Expected size: {expectSize}')\n",
    "print(f'Empirical size: {list(empSize)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jfWAkiWDWU7"
   },
   "source": [
    "### 2) Convolve an image of size 3x196x96 to produce a 5x66x49 result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XieXWJ9gDWU7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected size: [ 5 66 49]\n",
      "Empirical size: [5, 66, 49]\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "inChans  = 3\n",
    "imsize   = [196, 96]\n",
    "outChans = 5\n",
    "krnSize  = 5\n",
    "stride   = (3, 2)\n",
    "padding  = 3\n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1,inChans,imsize[0],imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans,0,0],dtype=int)\n",
    "expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n",
    "expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f'Expected size: {expectSize}')\n",
    "print(f'Empirical size: {list(empSize)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdRhRVE7FfN2"
   },
   "source": [
    "### 3) Convolve an image of size 1x32x32 to produce a 6x28x28 result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "t5f9x7HjFfN2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected size: [ 6 28 28]\n",
      "Empirical size: [6, 28, 28]\n"
     ]
    }
   ],
   "source": [
    "# note: these dimensions are the input -> first hidden layer of the famous LeNet-5\n",
    "\n",
    "# parameters\n",
    "inChans  = 1\n",
    "imsize   = [32, 32]\n",
    "outChans = 6\n",
    "krnSize  = 7\n",
    "stride   = (1, 1)\n",
    "padding  = 1\n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1,inChans,imsize[0],imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans,0,0],dtype=int)\n",
    "expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n",
    "expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f'Expected size: {expectSize}')\n",
    "print(f'Empirical size: {list(empSize)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrHM60CkF2pl"
   },
   "source": [
    "### 4) Convolve an image of size 3x227x227 to produce a 96x55x55 result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d0R0DITNF2pl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected size: [96 55 55]\n",
      "Empirical size: [96, 55, 55]\n"
     ]
    }
   ],
   "source": [
    "# note: these dimensions are the input -> first hidden layer of the famous AlexNet\n",
    "\n",
    "# parameters\n",
    "inChans  = 3\n",
    "imsize   = [227, 227]\n",
    "outChans = 96\n",
    "krnSize  = 13\n",
    "stride   = (4, 4)\n",
    "padding  = 1\n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1,inChans,imsize[0],imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans,0,0],dtype=int)\n",
    "expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n",
    "expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f'Expected size: {expectSize}')\n",
    "print(f'Empirical size: {list(empSize)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvoJ9X5IHtl5"
   },
   "source": [
    "### 5) Convolve an image of size 3x224x224 to produce a 64x224x224 result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PG4InNjQHtl6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected size: [ 64 224 224]\n",
      "Empirical size: [64, 224, 224]\n"
     ]
    }
   ],
   "source": [
    "# note: these dimensions are the input -> first hidden layer of the famous VGG-16\n",
    "\n",
    "# parameters\n",
    "inChans  = 3\n",
    "imsize   = [224, 224]\n",
    "outChans = 64\n",
    "krnSize  = 3\n",
    "stride   = (1, 1)\n",
    "padding  = 1\n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1,inChans,imsize[0],imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans,0,0],dtype=int)\n",
    "expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n",
    "expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f'Expected size: {expectSize}')\n",
    "print(f'Empirical size: {list(empSize)}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPOt/RlHgcfJ0cx2nA7NDMT",
   "collapsed_sections": [
    "mb0fBtyfEpsj"
   ],
   "name": "DUDL_convolution_codeChallenge.ipynb",
   "provenance": [
    {
     "file_id": "19imE5cZVTySOpFOe4Uvw97qMPMgs0MJ2",
     "timestamp": 1619520513379
    },
    {
     "file_id": "1GRajDS-VF5z8IslzZuMqbis3X6HDD-Uo",
     "timestamp": 1619468278654
    },
    {
     "file_id": "1m0n2-UmB2tJiIDadlFkE6L5A4iZSqeBf",
     "timestamp": 1619459134813
    },
    {
     "file_id": "19G9gTeBlYPQ-s3VS_3K2bVFtKTP344j6",
     "timestamp": 1619444797767
    },
    {
     "file_id": "1FcEBC0NAESIlHQkv6_85R-XDUKGE8XbM",
     "timestamp": 1619155961717
    },
    {
     "file_id": "1qKgZ8kVcqNgwtBzHbWq5yJH_HqI6DxWW",
     "timestamp": 1617803880910
    },
    {
     "file_id": "15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4",
     "timestamp": 1617737766196
    },
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617734878578
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
